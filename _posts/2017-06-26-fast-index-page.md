---
layout: post
date: 2017-06-26 20:15
title: "Ускорение индексации с помощью robots.txt"
description: Как с помощью правильной настройки robots.txt ускорить процесс индексации сайта? Правила использования директивы Clean-Param и рекомендации для веб-мастеров. 
mood: happy
category:
- seo
comments: true
---

<figure>
    <img src="http://dubkov.xyz/assets/img/robots.png" alt="Robots" />
</figure>

Медленно индексируется сайт в поисковых системах? У меня есть несколько рекомендаций по настройке правил для поисковых роботов robots.txt, которые в свою очередь помогут сохранить квоту краулинга для любых сайтов
от блогов и интернет-магазинов до порталов под управлением популярных CMS.

<!--more-->

## Краулинговый бюджет
#### Как работает crawler?

<b>Crawler</b> (веб-паук, краулер) — один из составляющих любой поисковой машины, краулер предназначен для поиска и внесения в собственную базу данных веб-страниц найденных на просторах всемирной паутины.
У каждого краулера есть квота для сайтов, которая ограничивается определенным количеством запросов в сутки для обхода новых и уже существующих страниц. Не каждому веб-мастеру это известно,
хотя данная информация давно была предоставлена специалистами из поисковых систем.

Секрет, как я упоминал выше, кроется в том, что необходимо сохранить данную квоту и не тратить ее на переобход ненужных страниц. «Приручить» веб-паука и заставить играть по своим правилам, можно с помощью дополнительных
инструкций и запрета индексации динамических параметров.

### Снижение количества обращений к сайту от поисковых систем
#### Почему это важно?

Способ обеспечения быстрой индексации сайта в поисковых системах Яндекс и Google, кроется в соблюдении простых правил, таких как:

* Грамонтная настройка правил robots.txt
* Анализ данных из панели веб-мастера

Поисковая система Яндекс за последний год, существенно улучшила инструментарий для российских веб-мастеров. Используйте данную возможность на полную катушку.

Яндекс.Вебмастер отдает в распоряжение владельцев сайтов, гораздо больше информации, чем в 2010-2012 годах. На основе статистики можно составить целую стратегию по увеличению скорости индексации.

Составление по-настоящему эффективных правил для поисковых машин базируется на запрете обхода ненужного контента, в который входят, такие вещи как: панель администратора, utm-метки, динамические данные 
и огромное количество других похожих страниц. Именно в запрете индексации страниц дублей и ненужного контента, кроется скорость индексации, потому как из 5 ежедневных запросов от поисковика, все 5 из них уйдут на просмотр наиболее важных страниц.

При настройке robots.txt, учитывайте <a href="https://yandex.ru/support/webmaster/controlling-robot/robots-txt.html" rel="nofollow">рекомендации</a> и правила от поисковых систем. Закройте от индексации: удаленные разделы, динамические параметры навигации (например, www.site.ru/latest?start=8), корзину товаров, поиск по сайту,
конфиденциальные данные страницы фильтров и сортировки, а так же другие нежелательные элементы, которые обычно отображаются в панели обхода страниц вашего сайта.  

### Директива Clean-Param
Динамическая директива Clean-Param является межсекционной, это означает, что ее можно использовать в любой строчке robots.txt и неограниченное число раз.
Если на сайте используются динамические параметры страниц, которые никак не влияют на их содержимое, мы можем закрыть такие параметры с помощью данной директивы.

#### Как использовать Clean-Param   

Для начала с помощью инструментов для веб-мастера необходимо отыскать динамические параметры. Они выглядят примерно так:

<figure>
    <img src="http://dubkov.xyz/assets/img/links-yawebmaster.png" alt="Яндекс.Вебмастер" />
    <figcaption>Скриншот из панели Яндекс.Вебмастер</figcaption>
</figure>

Как с помощью robots.txt запретить обход таких параметров? Если необходимо использовать запрет для конкретых директорий, пишем:

{% highlight html linenos %}Clean-param: start https://site.ru/news{% endhighlight %}
{% highlight html linenos %}Clean-param: start https://tolmax.ru/articles/featured{% endhighlight %}

Если динамических параметров несколько, воспользуемся амперсандом:

{% highlight html linenos %}Clean-param: format&type https://site.ru/evakuator-v-kaluge{% endhighlight %}

Если требуется отрезать параметр от всех страниц на сайте, достаточно написать в robots.txt:

{% highlight html linenos %}Clean-param: start{% endhighlight %}

Это же правило отлично работает для utm-меток:

{% highlight html linenos %}Clean-param: utm_source&utm_medium&utm_campaign{% endhighlight %}

Таким образом, вы существенно сэкономите бюджет обхода, тем самым, повысите обход наиболее важных и новых страниц на своем сайте. Внедряйте!